import logging
import os
import random
import warnings
from io import StringIO
from os.path import basename, exists, join
from statistics import mean

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import RMolEncoder as rme
import torch
import torch.nn as nn
from lithium import send_msg, send_photo
from torch.cuda.amp import GradScaler
from tqdm import tqdm

from __features import *
from __features import err_wrap
from _model import ChemLM, Loss, batch_reader, make_fix_len_collate, tokenize

# –ú–µ—Ç–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
send_msg(f'{__file__}: –†–∞—Å—Å—á–µ—Ç—ã –Ω–∞—á–∞–ª–∏—Å—å', delete_after=5)

dirname = '__train__'
if not exists(dirname):
    raise FileNotFoundError(f"–ù–µ—Ç –ø–∞–ø–∫–∏ —Å –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–æ–π –ø–æ –ø—É—Ç–∏: {dirname}")

pretrained_path = "pretrained_encoder.pth"
if not exists(pretrained_path):
    raise FileNotFoundError(f"–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {pretrained_path}")

log_filename = 'training.log'
logging.basicConfig(filename=log_filename, level=logging.INFO, format='%(message)s')
if exists(log_filename):
    with open(log_filename, 'w'):
        pass

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
warnings.filterwarnings("ignore", category=FutureWarning)


class Trainer:
    def __init__(self, model: ChemLM):
        self.model = model
        self.scaler = GradScaler()
        self.model.encoder.requires_grad_(False)
        self.model = torch.compile(self.model)
        self.model = self.model.to(device)
        self.losses = []
        self.optim = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–∫–∫—É–º—É–ª—è—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        self.virtual_batch_size = 256  # –ñ–µ–ª–∞–µ–º—ã–π –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        self.batch_size = 32           # –†–µ–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –ø–∞–º—è—Ç—å üî•üî•üî•
        self.grad_accum = self.virtual_batch_size // self.batch_size
        assert self.virtual_batch_size % self.batch_size == 0, "virtual_batch_size must be divisible by batch_size"
        self.accum_step = 0
        self.accum_losses = []

    def train(self, df: pd.DataFrame):
        MAX_SEQ_LEN = df["IUPAC Name"].apply(len).max()
        DS_SEQ_LEN = MAX_SEQ_LEN + 3
        smis = list(df['SMILES'])
        seqs = [tokenize(df.iloc[i]["IUPAC Name"], add_spezial_tokens=True) for i in range(df.shape[0])]

        ds = rme.dataset.RxnMolDataset(smis, seqs)
        dl = torch.utils.data.DataLoader(ds, batch_size=self.batch_size,
                                         collate_fn=make_fix_len_collate(DS_SEQ_LEN),
                                         shuffle=True, drop_last=False)
        losses = []
        for b in dl:
            x, y = b
            x, y = x.to(device), y.to(device)

            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
                p = self.model(x, y[:, :-1])
                loss = Loss(p, y[:, 1:])
                loss = loss / self.grad_accum  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º loss –¥–ª—è –∞–∫–∫—É–º—É–ª—è—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤

            self.accum_losses.append(float(loss) * self.grad_accum)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ loss
            self.scaler.scale(loss).backward()

            self.accum_step += 1
            if self.accum_step % self.grad_accum == 0:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º clipping –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–µ—Ä–µ–¥ —à–∞–≥–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                self.scaler.unscale_(self.optim)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                self.scaler.step(self.optim)
                self.scaler.update()
                self.optim.zero_grad()

                # –õ–æ–≥–∏—Ä—É–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π loss –∑–∞ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –±–∞—Ç—á
                avg_loss = np.mean(self.accum_losses)
                losses.append(avg_loss)
                self.accum_losses = []  # –û—á–∏—â–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ losses

        self.losses.extend(losses)
        return np.mean(losses) if losses else 0.0

    def validate(self, df: pd.DataFrame):
        self.model.eval()
        MAX_SEQ_LEN = df["IUPAC Name"].apply(len).max()
        DS_SEQ_LEN = MAX_SEQ_LEN + 3
        smis = list(df['SMILES'])
        seqs = [tokenize(df.iloc[i]["IUPAC Name"], add_spezial_tokens=True) for i in range(df.shape[0])]
        ds = rme.dataset.RxnMolDataset(smis, seqs)
        dl = torch.utils.data.DataLoader(ds, batch_size=self.batch_size,
                                         collate_fn=make_fix_len_collate(DS_SEQ_LEN),
                                         shuffle=False, drop_last=False)
        losses = []
        with torch.no_grad():
            for b in dl:
                x, y = b
                x, y = x.to(device), y.to(device)
                with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
                    p = self.model(x, y[:, :-1])
                    loss = Loss(p, y[:, 1:])
                losses.append(float(loss))
        self.model.train()
        return np.mean(losses)

    def send_result(self, telegram=True, *, local_save=False,
                    filename='training_.png', caption: str = "Training Loss",
                    xlabel='Iteration', ylabel='Loss', ylim: tuple = None):
        if telegram:
            send_photo(self.losses)
        if local_save:
            plt.figure()
            plt.plot(self.losses)
            if ylim:
                plt.ylim(ylim)
            plt.title(caption)
            plt.xlabel(xlabel)
            plt.ylabel(ylabel)
            plt.savefig(filename, format='png')
            plt.close()


def train(patience=10, batch_size=256, num_epoch=10):
    model = ChemLM(chem_encoder_params=dict(d_model=512, n_in_head=8, num_in_layers=8, shared_weights=True),
                   decoder_params=dict(d_model=512, nhead=8, dim_feedforward=4 * 512, dropout=0.1,
                                       activation=nn.functional.gelu, batch_first=True, norm_first=True, bias=True),
                   num_layers=4, vocab_size=128, chem_encoder_pretrained_path=pretrained_path)
    model.train()
    files = [join(dirname, 'train_Compound_000000001_000500000.csv')]
    validation_data = pd.read_csv(join('__val__', 'validation_Compound_000000001_000500000.csv'),
                                  delimiter=';', encoding='utf-8').dropna()
    loss = float('inf')
    best_val_loss = float('inf')
    epochs_without_improvement = 0
    for epoch in range(1, num_epoch):
        losses = []
        random.shuffle(files)
        trainer = Trainer(model)
        for file in files:
            def meta(): return f"{cpu()}, {ram()}, {gpu()}"
            with tqdm(total=count_lines(file), desc=meta(), dynamic_ncols=True) as tq:
                for num_batch, batch in enumerate(batch_reader(file, batch_size), start=1):
                    try:
                        df = pd.read_csv(StringIO(batch), delimiter=';', encoding='utf-8').dropna()
                    except Exception as exc:
                        logging.error(exc)
                        continue

                    loss = trainer.train(df)
                    losses.append(loss)
                    logging.info(
                        f"Epoch {epoch}, File: {basename(file)}, Batch {num_batch}, Loss: {loss:.4f}, MeanLoss: {mean(losses):.4f}")
                    tq.set_description(meta())
                    tq.update(batch_size)

        if mean(losses) < 0.1:
            logging.info(f"DONE")
            torch.save(model.state_dict(), "model.pth")
            break

        val_loss = trainer.validate(validation_data)
        logging.info(f"Epoch {epoch}, Validation loss: {val_loss:.4f}")
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            epochs_without_improvement = 0
            torch.save(model.state_dict(), "model.pth")
            logging.info(f"Epoch {epoch}: New best validation loss: {best_val_loss:.4f}")
        else:
            epochs_without_improvement += 1
            if epochs_without_improvement >= patience:
                logging.info(f"Early stopping triggered! No improvement for {patience} epochs.")
                break

        trainer.send_result(telegram=True, local_save=False)


if __name__ == '__main__':
    torch.cuda.empty_cache()
    try:
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
        best_model_state, filename = train(), "model.pth"
        send_msg(f'{__file__}: –†–∞—Å—Å—á–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω—ã', delete_after=5)
    except Exception as exc:
        base = basename(__file__)
        logging.error(err_wrap(base, exc))
        send_msg(f'{base}: –†–∞—Å—Å—á–µ—Ç—ã –∑–∞–∫–æ–Ω—á–µ–Ω—ã c –æ—à–∏–±–∫–æ–π', delete_after=5)
    finally:
        torch.cuda.empty_cache()
        if 'best_model_state' in locals():
            del best_model_state
